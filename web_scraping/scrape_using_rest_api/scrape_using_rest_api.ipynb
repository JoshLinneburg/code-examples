{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Dealership Scraping Tutorial Part 2: \n",
    "\n",
    "Hello! In this tutorial we will use Python to retrieve dealership data from the  website!\n",
    "\n",
    "What you'll need for this part of the tutorial (and all other parts): \n",
    "* Python3.3 or later\n",
    "* Jupyter Notebooks installed\n",
    "* A basic understanding of how websites retrieve and load content to users (or a willingness to learn!)\n",
    "* Patience to experiment!\n",
    "\n",
    "Because individual websites store, retrieve and load data (and by extension, content) to users in vastly different ways, this tutorial is not all-encompassing. What I mean is: While the rough methodology and experimental/scientific method outlined here will work for any site you come across, the code you will have to write to retrieve the data will be different. \n",
    "\n",
    "Depending on the site administration, you might sometimes get lucky. For instance, Chrsyler, Dodge, Jeep and Ram all use the same back-end [REST API](https://restfulapi.net/) for their United States dealer services. The only difference is the endpoint of the URL you'll want to make your requests to changes depending on whether you want Chrysler, Dodge, Jeep or Ram dealer results. \n",
    "\n",
    "In most cases, though, you'll find you're repeating the same experimental steps on every site you want to scrape. There will be some similarities that can be generalized, bits of code that can be made into functions and so on, but you'll always want to start with a fresh Jupyter Notebook to get yourself started (in my opinion).\n",
    "\n",
    "### A Skippable Tangent: Why Jupyter? \n",
    "\n",
    "You can skip this section to get right to the \"good\" stuff if you want. But I'd like to just say I think Jupyter Notebooks, for all their shortcomings as a proper IDE, are the best tool to use for jobs like this. \n",
    "\n",
    "The rapid experimentation and manipulation of objects (lists, dictionaries, etc.) that you can achieve in a Jupyter Notebook is unrivaled. The interactive nature of them provides a great environment to serve as your scratchpad from which you can copy and paste your \"solution\" into the IDE of your choice. \n",
    "\n",
    "There are some things an IDE like PyCharm will do you for: parameter previewing, underlining errors, proper debugging, and so on are just a few to name. But for just getting started, especially if this is your first time writing Python code, Jupyter is tough to beat. \n",
    "\n",
    "### Loading Libraries\n",
    "\n",
    "Okay! Let's load in our libraries. For this I'll be using:\n",
    "\n",
    "* json : the built-in Python JSON parsing/outputting library\n",
    "* requests : the best Python library for interacting with websites/APIs with [HTTP methods](https://www.w3schools.com/tags/ref_httpmethods.asp) (we'll use GET and POST exclusively)\n",
    "* mbtools : a custom-made module that has a bunch of useful functions for web scraping\n",
    "\n",
    "Unfortunately, the name of the author of mbtools has been lost to time. So I can't give them credit in this Notebook for their work. \n",
    "\n",
    "Notice how I'm importing `mbtools` from a different directory, `lib`, compared to the one I'm in, `scrape_unsecured_js_file`, so the import is structured like:\n",
    "\n",
    "```python\n",
    "import utils.mbtools as mbtools\n",
    "```\n",
    "\n",
    "This is possible because of the first code block below. What this does is it adds the `lib` directory that contains the `scraping_utils.py` and `mbtools.py` files to the PATH. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '../lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this works as you'd expect since this file can \"see\" the `lib` directory, even though it's in the `scrape_unsecured_js_file` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import mbtools\n",
    "from scraping_utils import show_obj_head as show_obj_head\n",
    "from scraping_utils import equivalence_checker as equivalence_checker\n",
    "import pprint # For pretty printing JSON-like objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep this in mind as you're moving custom module and utility files around: The project structure is very important and you'll spend a ton of time fixing path issues, relative import issues, etc. if you're not carful. \n",
    "\n",
    "If you'd like to see the `show_obj_head` and `equivalence_checker` code separately, take a look at the `scraping_utils.py` file located in the `lib` directory. This is where these helper functions are stored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Code Examples",
   "language": "python",
   "name": "code_examples"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
