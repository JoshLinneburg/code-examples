{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Dealership Scraping Tutorial Part 2: \n",
    "\n",
    "Hello! In this tutorial we will use Python to retrieve dealership data from the  website!\n",
    "\n",
    "What you'll need for this part of the tutorial (and all other parts): \n",
    "* Python3.3 or later\n",
    "* Jupyter Notebooks installed\n",
    "* A basic understanding of how websites retrieve and load content to users (or a willingness to learn!)\n",
    "* Patience to experiment!\n",
    "\n",
    "Because individual websites store, retrieve and load data (and by extension, content) to users in vastly different ways, this tutorial is not all-encompassing. What I mean is: While the rough methodology and experimental/scientific method outlined here will work for any site you come across, the code you will have to write to retrieve the data will be different. \n",
    "\n",
    "Depending on the site administration, you might sometimes get lucky. For instance, Chrsyler, Dodge, Jeep and Ram all use the same back-end [REST API](https://restfulapi.net/) for their United States dealer services. The only difference is the endpoint of the URL you'll want to make your requests to changes depending on whether you want Chrysler, Dodge, Jeep or Ram dealer results. \n",
    "\n",
    "In most cases, though, you'll find you're repeating the same experimental steps on every site you want to scrape. There will be some similarities that can be generalized, bits of code that can be made into functions and so on, but you'll always want to start with a fresh Jupyter Notebook to get yourself started (in my opinion)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Libraries\n",
    "\n",
    "Okay! Let's load in our libraries. For this I'll be using:\n",
    "\n",
    "* json : the built-in Python JSON parsing/outputting library\n",
    "* requests : the best Python library for interacting with websites/APIs with [HTTP methods](https://www.w3schools.com/tags/ref_httpmethods.asp) (we'll use GET and POST exclusively)\n",
    "* mbtools : a custom-made module that has a bunch of useful functions for web scraping\n",
    "\n",
    "Unfortunately, the name of the author of mbtools has been lost to time. So I can't give them credit in this Notebook for their work. \n",
    "\n",
    "Notice how I'm importing mbtools from a different directory, utils, compared to the one I'm in, scrape_unsecured_js_file, so the import is structured like:\n",
    "\n",
    "```python\n",
    "import utils.mbtools as mbtools\n",
    "```\n",
    "\n",
    "This is possible because of the first code block below. What this does is it adds the `lib` directory that contains the `example_utils.py` and `mbtools.py` files to the PATH. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Code Examples",
   "language": "python",
   "name": "code_examples"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
